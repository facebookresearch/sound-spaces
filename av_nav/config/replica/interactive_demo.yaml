BASE_TASK_CONFIG_PATH: "configs/replica/interactive_demo.yaml"
TRAINER_NAME: "ppo"
ENV_NAME: "NavRLEnv"
SIMULATOR_GPU_ID: 0
TORCH_GPU_ID: 0
TENSORBOARD_DIR: "tb"
VIDEO_DIR: "video_dir"
EVAL_CKPT_PATH_DIR: "data"
NUM_PROCESSES: 1
SENSORS: ["RGB_SENSOR"]
CHECKPOINT_FOLDER: "data"
# number of times updating the ppo agent
NUM_UPDATES: 300000
LOG_INTERVAL: 10
CHECKPOINT_INTERVAL: 50

TEST_EPISODE_COUNT: 500
VIDEO_OPTION: []
VISUALIZATION_OPTION: []

RL:
  PPO:
    # ppo params
    clip_param: 0.1
    ppo_epoch: 4
    num_mini_batch: 1
    value_loss_coef: 0.5
    entropy_coef: 0.20
    lr: 2.5e-4
    eps: 1e-5
    max_grad_norm: 0.5
    # decide the length of history that ppo encodes
    num_steps: 150
    hidden_size: 512
    use_gae: True
    gamma: 0.99
    tau: 0.95
    use_linear_clip_decay: True
    use_linear_lr_decay: True
    # window size for calculating the past rewards
    reward_window_size: 50

EVAL:
  SPLIT: "test_demo"
  USE_CKPT_CONFIG: True